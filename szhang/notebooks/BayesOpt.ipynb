{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#set up\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "## Math and dataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(X, y, clf=None, trn_weights = None):\n",
    "    \"\"\"General helper function for evaluating effectiveness of passed features in ML model\n",
    "    \n",
    "    Prints out Log loss, accuracy, and confusion matrix with 3-fold stratified cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Features array. Shape (n_samples, n_features)\n",
    "        \n",
    "        y (array-like): Labels array. Shape (n_samples,)\n",
    "        \n",
    "        clf: Classifier to use. If None, default Log reg is use.\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        raise ValueError(\"clf NOne\")\n",
    "    \n",
    "    probas = cross_val_predict(clf, X, y, cv=KFold(random_state=8), \n",
    "                              n_jobs=-1, method='predict_proba', verbose=2,\n",
    "                              fit_params = {'sample_weight': trn_weights}\n",
    "                              )\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.unique(y)\n",
    "    preds = classes[pred_indices]\n",
    "    print('Log loss: {}'.format(log_loss(y, probas)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, preds)))\n",
    "    print('F1 score: {}'.format(f1_score(y, preds)))\n",
    "    print('Auc score: {}'.format(roc_auc_score(y, preds)))\n",
    "    print( confusion_matrix(y, preds) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=20000, n_features=20,\n",
    "                                    n_informative=2, n_redundant=2)\n",
    "\n",
    "train_samples = 10000  # Samples used for training the models\n",
    "\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_test = y[train_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=3, n_estimators=100, learning_rate=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline xgb\n",
      "Log loss: 0.216402683828\n",
      "Accuracy: 0.91895\n",
      "F1 score: 0.918742794125\n",
      "Auc score: 0.918958775835\n",
      "[[9215  765]\n",
      " [ 856 9164]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    4.1s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"baseline xgb\")\n",
    "evaluate_features(X, y, clf=clf, trn_weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes opt\n",
    "def XGB_CV(\n",
    "          max_depth,\n",
    "          gamma,\n",
    "          min_child_weight,\n",
    "          max_delta_step,\n",
    "          subsample,\n",
    "          colsample_bytree\n",
    "         ):\n",
    "\n",
    "    global AUCbest\n",
    "    global ITERbest\n",
    "\n",
    "#\n",
    "# Define all XGboost parameters\n",
    "#\n",
    "\n",
    "    paramt = {\n",
    "              'booster' : 'gbtree',\n",
    "              'max_depth' : int(max_depth),\n",
    "              'gamma' : gamma,\n",
    "              'eta' : 0.1,\n",
    "              'objective' : 'binary:logistic',\n",
    "              'nthread' : 4,\n",
    "              'silent' : True,\n",
    "              'eval_metric': 'auc',\n",
    "              'subsample' : max(min(subsample, 1), 0),\n",
    "              'colsample_bytree' : max(min(colsample_bytree, 1), 0),\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'max_delta_step' : int(max_delta_step),\n",
    "              'seed' : 1001\n",
    "              }\n",
    "\n",
    "    folds = 5\n",
    "    cv_score = 0\n",
    "\n",
    "    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, paramt), file=log_file )\n",
    "    log_file.flush()\n",
    "\n",
    "    xgbc = xgb.cv(\n",
    "                    paramt,\n",
    "                    dtrain,\n",
    "                    num_boost_round = 20000,\n",
    "                    stratified = True,\n",
    "                    nfold = folds,\n",
    "#                    verbose_eval = 10,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    metrics = 'auc',\n",
    "                    show_stdv = True\n",
    "               )\n",
    "\n",
    "# This line would have been on top of this section\n",
    "#    with capture() as result:\n",
    "\n",
    "# After xgb.cv is done, this section puts its output into log file. Train and validation scores \n",
    "# are also extracted in this section. Note the \"diff\" part in the printout below, which is the \n",
    "# difference between the two scores. Large diff values may indicate that a particular set of \n",
    "# parameters is overfitting, especially if you check the CV portion of it in the log file and find \n",
    "# out that train scores were improving much faster than validation scores.\n",
    "\n",
    "#    print('', file=log_file)\n",
    "#    for line in result[1]:\n",
    "#        print(line, file=log_file)\n",
    "#    log_file.flush()\n",
    "\n",
    "    val_score = xgbc['test-auc-mean'].iloc[-1]\n",
    "    train_score = xgbc['train-auc-mean'].iloc[-1]\n",
    "    print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),\n",
    "(val_score*2-1)) )\n",
    "    if ( val_score > AUCbest ):\n",
    "        AUCbest = val_score\n",
    "        ITERbest = len(xgbc)\n",
    "\n",
    "    return (val_score*2) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open('Porto-AUC-5fold-XGB-run-01-v1-full.log', 'a')\n",
    "AUCbest = -1.\n",
    "ITERbest = 0\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "XGB_BO = BayesianOptimization(XGB_CV, {\n",
    "                                     'max_depth': (2, 12),\n",
    "                                     'gamma': (0.001, 10.0),\n",
    "                                     'min_child_weight': (0, 20),\n",
    "                                     'max_delta_step': (0, 10),\n",
    "                                     'subsample': (0.4, 1.0),\n",
    "                                     'colsample_bytree' :(0.4, 1.0)\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB_BO.explore({\n",
    "#               'max_depth':            [3, 8, 3, 8, 8, 3, 8, 3],\n",
    "#               'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "#               'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "#               'max_delta_step':       [1, 2, 2, 1, 2, 1, 1, 2],\n",
    "#               'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "#               'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "#               })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_delta_step |   max_depth |   min_child_weight |   subsample | \n",
      " Stopped after 64 iterations with train-auc = 0.976900 val-auc = 0.972154 ( diff = 0.004745 ) train-gini = 0.953800 val-gini = 0.944309\n",
      "    1 | 00m04s | \u001b[35m   0.94431\u001b[0m | \u001b[32m            0.6000\u001b[0m | \u001b[32m   0.5000\u001b[0m | \u001b[32m          1.0000\u001b[0m | \u001b[32m     3.0000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.6000\u001b[0m | \n",
      " Stopped after 41 iterations with train-auc = 0.982752 val-auc = 0.973108 ( diff = 0.009644 ) train-gini = 0.965503 val-gini = 0.946216\n",
      "    2 | 00m09s | \u001b[35m   0.94622\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   8.0000\u001b[0m | \u001b[32m          2.0000\u001b[0m | \u001b[32m     8.0000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.8000\u001b[0m | \n",
      " Stopped after 65 iterations with train-auc = 0.977875 val-auc = 0.972555 ( diff = 0.005320 ) train-gini = 0.955750 val-gini = 0.945110\n",
      "    3 | 00m03s |    0.94511 |             0.6000 |    0.2000 |           2.0000 |      3.0000 |             0.2000 |      0.6000 | \n",
      " Stopped after 48 iterations with train-auc = 0.980636 val-auc = 0.973212 ( diff = 0.007424 ) train-gini = 0.961273 val-gini = 0.946425\n",
      "    4 | 00m11s | \u001b[35m   0.94642\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   9.0000\u001b[0m | \u001b[32m          1.0000\u001b[0m | \u001b[32m     8.0000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.8000\u001b[0m | \n",
      " Stopped after 46 iterations with train-auc = 0.983297 val-auc = 0.971745 ( diff = 0.011552 ) train-gini = 0.966594 val-gini = 0.943490\n",
      "    5 | 00m06s |    0.94349 |             0.6000 |    0.5000 |           2.0000 |      8.0000 |            12.0000 |      0.6000 | \n",
      " Stopped after 132 iterations with train-auc = 0.977745 val-auc = 0.972034 ( diff = 0.005710 ) train-gini = 0.955489 val-gini = 0.944069\n",
      "    6 | 00m06s |    0.94407 |             0.8000 |    8.0000 |           1.0000 |      3.0000 |            12.0000 |      0.8000 | \n",
      " Stopped after 57 iterations with train-auc = 0.983822 val-auc = 0.971839 ( diff = 0.011983 ) train-gini = 0.967645 val-gini = 0.943678\n",
      "    7 | 00m06s |    0.94368 |             0.6000 |    0.2000 |           1.0000 |      8.0000 |            12.0000 |      0.6000 | \n",
      " Stopped after 113 iterations with train-auc = 0.976898 val-auc = 0.972262 ( diff = 0.004636 ) train-gini = 0.953796 val-gini = 0.944525\n",
      "    8 | 00m06s |    0.94452 |             0.8000 |    9.0000 |           2.0000 |      3.0000 |            12.0000 |      0.8000 | \n",
      " Stopped after 35 iterations with train-auc = 0.980198 val-auc = 0.972514 ( diff = 0.007683 ) train-gini = 0.960395 val-gini = 0.945028\n",
      "    9 | 00m07s |    0.94503 |             0.8672 |    3.3482 |           0.1488 |      7.6907 |            17.0864 |      0.8012 | \n",
      " Stopped after 65 iterations with train-auc = 0.977011 val-auc = 0.972251 ( diff = 0.004760 ) train-gini = 0.954022 val-gini = 0.944503\n",
      "   10 | 00m05s |    0.94450 |             0.9450 |    4.8188 |           6.3199 |      3.1324 |            11.2750 |      0.9875 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_delta_step |   max_depth |   min_child_weight |   subsample | \n",
      " Stopped after 301 iterations with train-auc = 0.976916 val-auc = 0.972081 ( diff = 0.004835 ) train-gini = 0.953833 val-gini = 0.944163\n",
      "   11 | 00m53s |    0.94416 |             0.9807 |    9.4595 |           8.8379 |      2.2040 |             0.0571 |      0.8182 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saizhang/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-2.47200369e-05]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stopped after 77 iterations with train-auc = 0.979714 val-auc = 0.972963 ( diff = 0.006751 ) train-gini = 0.959428 val-gini = 0.945925\n",
      "   12 | 00m52s |    0.94593 |             0.9854 |    9.9495 |           0.5661 |     11.6228 |            14.8761 |      0.9928 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saizhang/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([4.27628029e-05]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stopped after 109 iterations with train-auc = 0.974641 val-auc = 0.971362 ( diff = 0.003280 ) train-gini = 0.949283 val-gini = 0.942724\n",
      "   13 | 01m37s |    0.94272 |             0.8768 |    9.5930 |           2.3707 |      2.0212 |            19.8594 |      0.9324 | \n",
      " Stopped after 68 iterations with train-auc = 0.978659 val-auc = 0.972368 ( diff = 0.006291 ) train-gini = 0.957318 val-gini = 0.944736\n",
      "   14 | 01m13s |    0.94474 |             0.8413 |    0.1335 |           6.7944 |      3.9460 |             0.1392 |      0.9714 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saizhang/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-3.89883608e-05]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stopped after 63 iterations with train-auc = 0.980602 val-auc = 0.972782 ( diff = 0.007821 ) train-gini = 0.961205 val-gini = 0.945564\n",
      "   15 | 00m54s |    0.94556 |             0.9984 |    9.8275 |           0.4670 |     10.9794 |             2.0849 |      0.4713 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saizhang/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00030191]), 'nit': 6, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results\n",
      "Maximum XGBOOST value: 0.946425\n",
      "Best XGBOOST parameters:  {'colsample_bytree': 0.8, 'max_delta_step': 1.0, 'min_child_weight': 0.2, 'subsample': 0.8, 'max_depth': 8.0, 'gamma': 9.0}\n"
     ]
    }
   ],
   "source": [
    "XGB_BO.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)\n",
    "\n",
    "print('Final Results')\n",
    "print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'])\n",
    "print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'])\n",
    "print('-'*130, file=log_file)\n",
    "print('Final Result:', file=log_file)\n",
    "print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'], file=log_file)\n",
    "print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'], file=log_file)\n",
    "\n",
    "history_df = pd.DataFrame(XGB_BO.res['all']['params'])\n",
    "history_df2 = pd.DataFrame(XGB_BO.res['all']['values'])\n",
    "history_df = pd.concat((history_df, history_df2), axis=1)\n",
    "history_df.rename(columns = { 0 : 'gini'}, inplace=True)\n",
    "history_df['AUC'] = ( history_df['gini'] + 1 ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
