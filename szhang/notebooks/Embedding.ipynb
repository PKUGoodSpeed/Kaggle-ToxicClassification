{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "## Math and dataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, confusion_matrix\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set len  159571\n",
      "test set len  153164\n",
      "clean samples 143346\n",
      "toxic samples 16225\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene',  'threat', 'insult', 'identity_hate']\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train['dirtyness'] = train.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "test['dirtyness'] = test.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "print(\"train set len \", len(train) )\n",
    "print(\"test set len \", len(test) )\n",
    "print(\"clean samples\", len(train[train['dirtyness'] == 0]))\n",
    "print(\"toxic samples\", len(train[train['dirtyness'] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 1024\n",
    "embedding_dims = 50\n",
    "filters = 120\n",
    "kernel_size = 3\n",
    "hidden_dims = 50\n",
    "epochs = 2\n",
    "\n",
    "#preprocessing\n",
    "raw_text = np.hstack([train.comment_text.str.lower(), test.comment_text.str.lower()])\n",
    "tok_raw = Tokenizer(num_words=max_features)\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "train[\"seq\"] = tok_raw.texts_to_sequences(train.comment_text.str.lower())\n",
    "test[\"seq\"] = tok_raw.texts_to_sequences(test.comment_text.str.lower())\n",
    "\n",
    "#sequence.pad_sequences(train['comment_text'].values, maxlen=maxlen)\n",
    "#test['comment_text'] = sequence.pad_sequences(test['comment_text'], maxlen=maxlen)\n",
    "#print('x_train shape:', train['comment_text'].values.shape)\n",
    "#print('x_test shape:', test['comment_text'].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad\n",
    "#train[\"seq\"].apply(lambda x: len(x)).describe()\n",
    "train[\"seq_pad\"] = train[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n",
    "test[\"seq_pad\"] = test[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 98, 120)           18120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_44 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,024,221\n",
      "Trainable params: 1,024,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_59 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 98, 120)           18120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_45 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,024,221\n",
      "Trainable params: 1,024,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_60 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 98, 120)           18120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_46 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,024,221\n",
      "Trainable params: 1,024,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_61 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 98, 120)           18120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_47 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,024,221\n",
      "Trainable params: 1,024,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_62 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 98, 120)           18120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_48 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,024,221\n",
      "Trainable params: 1,024,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_63 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 98, 120)           18120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_49 (Glo (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,024,221\n",
      "Trainable params: 1,024,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "def buildModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # define metrics and compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "modeldict = {}\n",
    "for lc in label_cols:\n",
    "    modeldict[lc] = buildModel()\n",
    "    modeldict[lc].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load fe , we will use resample to balance the labels\n",
    "from models.FeatureExtraction import FeatureExtraction\n",
    "\n",
    "fe = FeatureExtraction()\n",
    "\n",
    "trainOrig = np.array(train['seq_pad'].tolist())\n",
    "assert(trainOrig.shape == (len(train), maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8941\n",
      "Epoch 00001: val_acc improved from -inf to 0.92030, saving model to toxic_weights_best.hdf5\n",
      "106912/106912 [==============================] - 66s 614us/step - loss: 0.3413 - acc: 0.8941 - val_loss: 0.2268 - val_acc: 0.9203\n",
      "Epoch 2/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9510\n",
      "Epoch 00002: val_acc improved from 0.92030 to 0.95742, saving model to toxic_weights_best.hdf5\n",
      "106912/106912 [==============================] - 60s 559us/step - loss: 0.1384 - acc: 0.9510 - val_loss: 0.1178 - val_acc: 0.9574\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9801\n",
      "Epoch 00001: val_acc improved from -inf to 0.99001, saving model to severe_toxic_weights_best.hdf5\n",
      "106912/106912 [==============================] - 63s 587us/step - loss: 0.1600 - acc: 0.9802 - val_loss: 0.0498 - val_acc: 0.9900\n",
      "Epoch 2/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9900\n",
      "Epoch 00002: val_acc improved from 0.99001 to 0.99022, saving model to severe_toxic_weights_best.hdf5\n",
      "106912/106912 [==============================] - 59s 551us/step - loss: 0.0467 - acc: 0.9900 - val_loss: 0.0365 - val_acc: 0.9902\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9474\n",
      "Epoch 00001: val_acc improved from -inf to 0.94624, saving model to obscene_weights_best.hdf5\n",
      "106912/106912 [==============================] - 62s 582us/step - loss: 0.2445 - acc: 0.9474 - val_loss: 0.1691 - val_acc: 0.9462\n",
      "Epoch 2/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9696\n",
      "Epoch 00002: val_acc improved from 0.94624 to 0.97858, saving model to obscene_weights_best.hdf5\n",
      "106912/106912 [==============================] - 59s 554us/step - loss: 0.0915 - acc: 0.9696 - val_loss: 0.0614 - val_acc: 0.9786\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9917\n",
      "Epoch 00001: val_acc improved from -inf to 0.99711, saving model to threat_weights_best.hdf5\n",
      "106912/106912 [==============================] - 63s 586us/step - loss: 0.1096 - acc: 0.9917 - val_loss: 0.0202 - val_acc: 0.9971\n",
      "Epoch 2/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9970\n",
      "Epoch 00002: val_acc did not improve\n",
      "106912/106912 [==============================] - 57s 529us/step - loss: 0.0205 - acc: 0.9970 - val_loss: 0.0192 - val_acc: 0.9971\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9506\n",
      "Epoch 00001: val_acc improved from -inf to 0.94981, saving model to insult_weights_best.hdf5\n",
      "106912/106912 [==============================] - 65s 604us/step - loss: 0.2280 - acc: 0.9507 - val_loss: 0.1643 - val_acc: 0.9498\n",
      "Epoch 2/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9621\n",
      "Epoch 00002: val_acc improved from 0.94981 to 0.97013, saving model to insult_weights_best.hdf5\n",
      "106912/106912 [==============================] - 60s 559us/step - loss: 0.1093 - acc: 0.9622 - val_loss: 0.0790 - val_acc: 0.9701\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9877\n",
      "Epoch 00001: val_acc improved from -inf to 0.99106, saving model to identity_hate_weights_best.hdf5\n",
      "106912/106912 [==============================] - 64s 595us/step - loss: 0.1377 - acc: 0.9877 - val_loss: 0.0489 - val_acc: 0.9911\n",
      "Epoch 2/2\n",
      "106496/106912 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9913\n",
      "Epoch 00002: val_acc improved from 0.99106 to 0.99109, saving model to identity_hate_weights_best.hdf5\n",
      "106912/106912 [==============================] - 58s 538us/step - loss: 0.0474 - acc: 0.9913 - val_loss: 0.0436 - val_acc: 0.9911\n"
     ]
    }
   ],
   "source": [
    "#define F1 metric\n",
    "import keras\n",
    "#fit\n",
    "reSample = False\n",
    "\n",
    "for lc in label_cols:\n",
    "    # train test split\n",
    "    if reSample == True:\n",
    "        trn_re, label_re = fe.reSample( scipy.sparse.csr_matrix(trainOrig) , y = train[lc])\n",
    "    else:\n",
    "        trn_re, label_re = scipy.sparse.csr_matrix(trainOrig), train[lc].values\n",
    "    x_train, x_val, y_train, y_val = train_test_split(trn_re, label_re, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # add check point\n",
    "    filepath= lc + \"_weights_best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    modeldict[lc].fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val), \n",
    "          callbacks=callbacks_list\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implement Trained Model\n",
    "from models.TrainedModel import TrainedModel\n",
    "import modelDB\n",
    "\n",
    "class TrainedModelCNNEmbedding(TrainedModel):\n",
    "    def __init__(self, md = None):\n",
    "        super(TrainedModelCNNEmbedding, self).__init__(md)\n",
    "    \n",
    "    def predict(self, test, **kwargs):\n",
    "        print (\"Predict using Model: \")\n",
    "        for i in self.md.keys():  \n",
    "            print( \"{} - {}\".format(i, type(self.md[i]).__name__ ) )\n",
    "            \n",
    "        res = {}\n",
    "        for topic, submd in zip( self.md.keys(), self.md.values() ):\n",
    "            print( \" predicitng topic {}\".format(topic))\n",
    "            res[topic] = zip(*submd.predict(test, batch_size = 1024))[0] #obtain proba\n",
    "        \n",
    "        #important to keep the order as required by the submission file\n",
    "        testid = kwargs['testid'] \n",
    "        #print (testid)\n",
    "        dfres = pd.DataFrame(res)\n",
    "        dfres['id'] = testid\n",
    "        \n",
    "        #print(dfres.shape)\n",
    "        assert(dfres.shape[0] == test.shape[0])\n",
    "        \n",
    "        #reshape to submission file format\n",
    "        dfres = dfres[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "        \n",
    "        return dfres\n",
    "    \n",
    "    def _save(self, mDB, nameKey, modelpath, **kwargs):\n",
    "        '''\n",
    "        save model into model data base\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        print(\"Saving model\")\n",
    "        import datetime\n",
    "        \n",
    "        #save a dict (topic => ('modelname', 'weightname')) into db\n",
    "        model_saved_toDB = {}\n",
    "        for topic, submd in zip( self.md.keys(), self.md.values() ):\n",
    "            #this is the file we will save model to\n",
    "            mdname = os.path.join(modelpath, nameKey + '_'+ topic +'_' + '.sav' )\n",
    "            mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "\n",
    "            #this is the file we will save weights to\n",
    "            weightName = os.path.join(modelpath, nameKey +  '_'+ topic + '_' + '_weights.h5' )\n",
    "            weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "\n",
    "            #convert md to json and save to file\n",
    "            print(\" Saving model {}\".format(mdname) )\n",
    "            model_json = submd.to_json()\n",
    "            with open(mdnameAbs, \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "\n",
    "            # serialize weights to HDF5\n",
    "            print(\" Saving weights {}\".format(weightName) )\n",
    "            submd.save_weights(weightNameAbs)\n",
    "    \n",
    "            model_saved_toDB[topic] = (mdname, weightName)\n",
    "            \n",
    "        \n",
    "        #db schema\n",
    "        # 'modelName', type {rnn, cnn, rf}, date, model\n",
    "        print( \"Info: custom saving options\" )\n",
    "        for i in kwargs:\n",
    "            print(\"{} - {}\".format(i , kwargs[i]))\n",
    "            \n",
    "        import json\n",
    "        newRow = pd.DataFrame({\n",
    "            'modelName': [nameKey],\n",
    "            'type': kwargs['modelType'],\n",
    "            'subType': kwargs['modelSubType'],\n",
    "            'date': str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "            'model' : json.dumps(model_saved_toDB)\n",
    "        }\n",
    "        )\n",
    "        \n",
    "        #add a new row \n",
    "        #mDB = pd.concat([mDB, newRow])\n",
    "        mDB = mDB.append(newRow, ignore_index = True)\n",
    "        display(mDB)\n",
    "\n",
    "        return mDB\n",
    "    \n",
    "    def load(self, mDB, nameKey, modelpath):\n",
    "        '''\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        from keras.models import model_from_json\n",
    "        import json\n",
    "        \n",
    "        if (mDB.empty) or mDB[ mDB['modelName'] == nameKey ].empty:\n",
    "            raise VaueError(\"Model name does not exist\")\n",
    "        print(\"loadModel\")\n",
    "        #the saved model is of format: dict (topic => ('modelname', 'weightname')) into db\n",
    "        assert(len(mDB[mDB['modelName'] == nameKey]) == 1)\n",
    "        saved_model_inDB = json.loads(mDB[mDB['modelName'] == nameKey].iloc[0]['model'])\n",
    "        \n",
    "        print(\"saved json string representing the model is {}\".format(saved_model_inDB))\n",
    "        loaded_model = {}\n",
    "        for topic, mdPointer in saved_model_inDB.iteritems():\n",
    "            print(\" load model for topic {}\".format(topic))\n",
    "            mdname = mdPointer[0]\n",
    "            mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "            print(\"  model file in {}\".format(mdnameAbs))\n",
    "\n",
    "            #this is the file we will save weights to\n",
    "            weightName = mdPointer[1]\n",
    "            weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "            print(\"  weight file in {}\".format(weightNameAbs))\n",
    "            \n",
    "            with open(mdnameAbs, 'r') as json_file:\n",
    "                tmpModel_json = json_file.read()\n",
    "            tmpModel = model_from_json(tmpModel_json)\n",
    "            #load weights\n",
    "            tmpModel.load_weights(weightNameAbs)\n",
    "            \n",
    "            #assign the model into loaded model dict\n",
    "            loaded_model[topic] = tmpModel\n",
    "        \n",
    "        assert(self.md is None)\n",
    "        self.setModel(loaded_model)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Utest test predict\n",
    "# myCNN1d = TrainedModelCNNEmbedding(modeldict)\n",
    "# dfres = myCNN1d.predict(trainOrig[0:1000, :], testid = train['id'])\n",
    "# display(dfres.head(20))\n",
    "# display(train.head(20)[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']])\n",
    "\n",
    "# #Utest test save\n",
    "# mdDB = pd.read_json(\"../modelDB/modelMetaDB.json\")\n",
    "# modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "# print(\"current modelDB\")\n",
    "# display(mdDB)\n",
    "\n",
    "# mdDB = myCNN1d.save(mdDB, 'CNN1d-Embedding_Random-OneperClass', modelpath, modelType = 'CNN1d', modelSubType = 'Embedding_Random')\n",
    "# #mdDB.to_json(\"../modelDB/modelMetaDB.json\")\n",
    "\n",
    "# #Utest test load\n",
    "# loadedCNN = TrainedModelCNNEmbedding().load( mdDB, 'CNN1d-Embedding_Random-OneperClass', modelpath)\n",
    "# for i in loadedCNN.md.keys():  \n",
    "#     print( \"{} - {}\".format(i, loadedCNN.md[i] ) )\n",
    "# dfres_loaded = loadedCNN.predict(trainOrig[0:1000, :], testid = train['id'])\n",
    "# assert(dfres_loaded.equals(dfres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict using Model: \n",
      "severe_toxic - Sequential\n",
      "identity_hate - Sequential\n",
      "obscene - Sequential\n",
      "insult - Sequential\n",
      "threat - Sequential\n",
      "toxic - Sequential\n",
      " predicitng topic severe_toxic\n",
      " predicitng topic identity_hate\n",
      " predicitng topic obscene\n",
      " predicitng topic insult\n",
      " predicitng topic threat\n",
      " predicitng topic toxic\n"
     ]
    }
   ],
   "source": [
    "#forward pass to inference\n",
    "testOrig = np.array(test['seq_pad'].tolist())\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbedding(modeldict)\n",
    "\n",
    "dfres = myCNN1d.predict(testOrig, testid = test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      " Saving model cnn/CNN1d-Embedding_Random-OneperClass_arch2_severe_toxic_.sav\n",
      " Saving weights cnn/CNN1d-Embedding_Random-OneperClass_arch2_severe_toxic__weights.h5\n",
      " Saving model cnn/CNN1d-Embedding_Random-OneperClass_arch2_identity_hate_.sav\n",
      " Saving weights cnn/CNN1d-Embedding_Random-OneperClass_arch2_identity_hate__weights.h5\n",
      " Saving model cnn/CNN1d-Embedding_Random-OneperClass_arch2_obscene_.sav\n",
      " Saving weights cnn/CNN1d-Embedding_Random-OneperClass_arch2_obscene__weights.h5\n",
      " Saving model cnn/CNN1d-Embedding_Random-OneperClass_arch2_insult_.sav\n",
      " Saving weights cnn/CNN1d-Embedding_Random-OneperClass_arch2_insult__weights.h5\n",
      " Saving model cnn/CNN1d-Embedding_Random-OneperClass_arch2_threat_.sav\n",
      " Saving weights cnn/CNN1d-Embedding_Random-OneperClass_arch2_threat__weights.h5\n",
      " Saving model cnn/CNN1d-Embedding_Random-OneperClass_arch2_toxic_.sav\n",
      " Saving weights cnn/CNN1d-Embedding_Random-OneperClass_arch2_toxic__weights.h5\n",
      "Info: custom saving options\n",
      "modelSubType - Embedding_Random\n",
      "modelType - CNN1d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model</th>\n",
       "      <th>modelName</th>\n",
       "      <th>subType</th>\n",
       "      <th>type</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>cnn/utestModel.sav</td>\n",
       "      <td>utestModel</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>testType</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass_arch1</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>svc/SVC_tfidf_1.sav</td>\n",
       "      <td>SVC_tfidf_1</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>svc/SVC_keywordTermFreq_1.sav</td>\n",
       "      <td>SVC_keywordTermFreq_1</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass_arch2</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              model  \\\n",
       "0  1970-01-01 00:00:00  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "1  1970-01-01 00:00:00                                 cnn/utestModel.sav   \n",
       "2           2018-02-13  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "3           2018-02-13                                svc/SVC_tfidf_1.sav   \n",
       "4           2018-02-13                      svc/SVC_keywordTermFreq_1.sav   \n",
       "5           2018-02-14  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "\n",
       "                                  modelName           subType      type  \\\n",
       "0        CNN1d-Embedding_Random-OneperClass  Embedding_Random     CNN1d   \n",
       "1                                utestModel             TfIdf  testType   \n",
       "2  CNN1d-Embedding_Random-OneperClass_arch1  Embedding_Random     CNN1d   \n",
       "3                               SVC_tfidf_1             TfIdf       SVC   \n",
       "4                     SVC_keywordTermFreq_1             TfIdf       SVC   \n",
       "5  CNN1d-Embedding_Random-OneperClass_arch2  Embedding_Random     CNN1d   \n",
       "\n",
       "   weights  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "5      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save\n",
    "myCNN1d = TrainedModelCNNEmbedding(modeldict)\n",
    "\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "#print(\"current modelDB\")\n",
    "#display(mdDB)\n",
    "mdDB = myCNN1d.save(mdDB, 'CNN1d-Embedding_Random-OneperClass_arch2', modelpath, modelType = 'CNN1d', modelSubType = 'Embedding_Random')\n",
    "mdDB.to_pickle(\"../modelDB/modelMetaDB.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "dfres.to_csv('../submission/submission_cnn1d_arch2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
