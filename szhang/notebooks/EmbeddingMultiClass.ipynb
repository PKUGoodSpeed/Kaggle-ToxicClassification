{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "## Math and dataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, confusion_matrix\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set len  159571\n",
      "test set len  153164\n",
      "clean samples 143346\n",
      "toxic samples 16225\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene',  'threat', 'insult', 'identity_hate']\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train['dirtyness'] = train.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "test['dirtyness'] = test.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "print(\"train set len \", len(train) )\n",
    "print(\"test set len \", len(test) )\n",
    "print(\"clean samples\", len(train[train['dirtyness'] == 0]))\n",
    "print(\"toxic samples\", len(train[train['dirtyness'] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "modelType = 'LSTM'\n",
    "modelSubTypeList = ['MultiClass_Embedding_Random', 'tfIdf']\n",
    "\n",
    "def secondInput(modelSubTypeList): \n",
    "    if len(modelSubTypeList) == 1:\n",
    "        return None\n",
    "    else:\n",
    "        return modelSubTypeList[1]\n",
    "    \n",
    "modelSubType = '+'.join(modelSubTypeList)\n",
    "reSample = False\n",
    "runNumber = 1\n",
    "modelName = modelType + '_' + modelSubType + '_' + str(reSample) + '_arch' + str(runNumber)\n",
    "\n",
    "#hyper param\n",
    "batch_size = 64\n",
    "epochs = 8\n",
    "\n",
    "# tokenize\n",
    "max_features = 20000\n",
    "maxlen = 120\n",
    "embedding_dims = 10\n",
    "filters = 12\n",
    "hidden_dims = 2\n",
    "kernel_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "raw_text = np.hstack([train.comment_text.str.lower(), test.comment_text.str.lower()])\n",
    "tok_raw = Tokenizer(num_words=max_features)\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "train[\"seq\"] = tok_raw.texts_to_sequences(train.comment_text.str.lower())\n",
    "test[\"seq\"] = tok_raw.texts_to_sequences(test.comment_text.str.lower())\n",
    "\n",
    "#sequence.pad_sequences(train['comment_text'].values, maxlen=maxlen)\n",
    "#test['comment_text'] = sequence.pad_sequences(test['comment_text'], maxlen=maxlen)\n",
    "#print('x_train shape:', train['comment_text'].values.shape)\n",
    "#print('x_test shape:', test['comment_text'].values.shape)\n",
    "#pad\n",
    "#train[\"seq\"].apply(lambda x: len(x)).describe()\n",
    "train[\"seq_pad\"] = train[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n",
    "test[\"seq_pad\"] = test[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> EmbMatrix of type <type 'NoneType'>\n",
      ">> 2ndInput of type <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# FE factory\n",
    "def FEEmbedding(EmbeddingStrategy):\n",
    "    ''' returns embedding matrix\n",
    "    '''\n",
    "    if EmbeddingStrategy == 'MultiClass_Embedding_Random':\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(\"Undefined embedding strategy\")\n",
    "        \n",
    "def secondFE(secondFEStrategy):\n",
    "    trn_2nd_input = None\n",
    "    test_2nd_input = None\n",
    "    aux_input_dim = None\n",
    "    from models.FeatureExtraction import FeatureExtraction\n",
    "    fe = FeatureExtraction()    \n",
    "    \n",
    "    if secondFEStrategy == 'KeyWordTermFreq':\n",
    "\n",
    "\n",
    "        keyword_dir = '../../ZhiHaoSun/'\n",
    "        keyfiles = [\n",
    "                keyword_dir + 'toxic_words.txt',\n",
    "                keyword_dir + 'identity_hate_words.txt',\n",
    "                keyword_dir + 'insult_words.txt',\n",
    "                keyword_dir + 'obscene_words.txt',\n",
    "                keyword_dir + 'threat_words.txt',\n",
    "                keyword_dir + 'identity_hate_words.txt',\n",
    "                ]\n",
    "\n",
    "        term_doc = fe.tfKeyWordEnsemble(\n",
    "                pd.concat([train, test]), n_feature = 80000, vocabfile = keyfiles,\n",
    "                COMMENT = 'comment_text'\n",
    "                )\n",
    "\n",
    "        trn_term_doc = term_doc.tocsr()[0:len(train), :]\n",
    "        test_term_doc = term_doc.tocsr()[len(train)::, :]\n",
    "        trn_2nd_input = trn_term_doc\n",
    "        test_2nd_input = test_term_doc\n",
    "    \n",
    "        aux_input_dim = trn_2nd_input.shape[1]\n",
    "        \n",
    "    elif secondFEStrategy == \"tfIdf\":\n",
    "        term_doc = fe.tfIdf(pd.concat([train, test]), 'comment_text')\n",
    "        trn_term_doc = term_doc.tocsr()[0:len(train), :]\n",
    "        test_term_doc = term_doc.tocsr()[len(train)::, :]\n",
    "        trn_2nd_input = trn_term_doc\n",
    "        test_2nd_input = test_term_doc\n",
    "\n",
    "        aux_input_dim = trn_2nd_input.shape[1]\n",
    "        \n",
    "    return trn_2nd_input, test_2nd_input, aux_input_dim\n",
    "\n",
    "\n",
    "embMatrix = FEEmbedding(modelSubTypeList[0])\n",
    "print(\">> EmbMatrix of type {}\".format(type(embMatrix)))\n",
    "\n",
    "trn_2nd_input, test_2nd_input, aux_input_dim = secondFE(modelSubTypeList[1])\n",
    "print(\">> 2ndInput of type {}\".format(type(trn_2nd_input)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 689099)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_48 (Embedding)        (None, 120, 10)      200000      input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 4)            2756400     aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_36 (Bidirectional (None, 120, 4)       208         embedding_48[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4)            16          dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_47 (Global (None, 4)            0           bidirectional_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4)            0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4)            16          global_max_pooling1d_47[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 4)            0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8)            0           batch_normalization_33[0][0]     \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 2)            18          concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 2)            0           dense_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 6)            18          dropout_62[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,956,676\n",
      "Trainable params: 2,956,660\n",
      "Non-trainable params: 16\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model factory\n",
    "def buildModel(modelType):\n",
    "    if modelType == 'CNN1d':\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        model.add(Embedding(max_features,\n",
    "                            embedding_dims,\n",
    "                            input_length=maxlen, \n",
    "                            #embeddings_regularizer = keras.regularizers.l1(0.01)\n",
    "                            ))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # we add a Convolution1D, which will learn filters\n",
    "        # word group filters of size filter_length:\n",
    "        model.add(Conv1D(filters,\n",
    "                         kernel_size,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(BatchNormalization())\n",
    "        # we use max pooling:\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "\n",
    "        # We add a vanilla hidden layer:\n",
    "        model.add(Dense(hidden_dims))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "        model.add(Dense(6))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        # define metrics and compile model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    elif (modelType == 'LSTM') and (secondInput(modelSubTypeList) is None) :\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embedding_dims)(inp)\n",
    "        x = Bidirectional(LSTM(hidden_dims, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        #x = Dense(hidden_dims, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    elif (modelType == 'LSTM') and (secondInput(modelSubTypeList) is not None):\n",
    "        \n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embedding_dims)(inp)\n",
    "        x = Bidirectional(LSTM(hidden_dims, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        auxiliary_input = Input(shape=(aux_input_dim,), name='aux_input')\n",
    "        y = Dense(4)(auxiliary_input)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "\n",
    "        x = keras.layers.concatenate([x, y])\n",
    "\n",
    "        x = Dense(hidden_dims, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        x = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=[inp, auxiliary_input], outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"undefined model type\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(modelType)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not reSample to balance\n"
     ]
    }
   ],
   "source": [
    "# reSample factory\n",
    "def reSampleToBalance(reSample, train, trn_2nd_input):\n",
    "    \n",
    "    trainOrig = np.array(train['seq_pad'].tolist())\n",
    "    assert(trainOrig.shape == (len(train), maxlen))\n",
    "    train2ndOrig = trn_2nd_input\n",
    "    trn_weights = None\n",
    "    \n",
    "    if reSample == True:\n",
    "        print(\"reSample to balance\")\n",
    "        assert((secondInput(modelSubTypeList) is None))\n",
    "        trn_re, label_re = fe.reSample( scipy.sparse.csr_matrix(trainOrig) , y = train[label_cols].values)\n",
    "        \n",
    "    elif reSample == False:\n",
    "        print(\"Not reSample to balance\")\n",
    "        trn_re, label_re = scipy.sparse.csr_matrix(trainOrig), train[label_cols].values\n",
    "        train2nd_re = train2ndOrig\n",
    "        \n",
    "    elif reSample == \"covCorrection\":\n",
    "        print(\"covCorrection\")\n",
    "        trn_re, label_re = scipy.sparse.csr_matrix(trainOrig), train[label_cols].values\n",
    "        train2nd_re = train2ndOrig\n",
    "\n",
    "        term_doc = fe.tfIdf(pd.concat([train, test]), 'comment_text')\n",
    "        trn_term_doc = term_doc.tocsr()[0:len(train), :]\n",
    "        test_term_doc = term_doc.tocsr()[len(train)::, :]\n",
    "\n",
    "        trn_weights = fe.covarianceShiftCorrection(trn_term_doc, test_term_doc)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Undefined reSample strategy\")\n",
    "    \n",
    "    return trn_re, label_re, train2nd_re, trn_weights\n",
    "\n",
    "\n",
    "train_re, label_re, train2nd_re, trn_weights = reSampleToBalance(reSample, train, trn_2nd_input)\n",
    "\n",
    "y_train = label_re\n",
    "if secondInput(modelSubTypeList) is not None:\n",
    "    assert(train2nd_re is not None)\n",
    "    x_train = [train_re, train2nd_re]\n",
    "else:\n",
    "    x_train = train_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add check point and fit\n",
    "import keras\n",
    "filepath= type(model).__name__ + \"_weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "esCallback = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "callbacks_list = [checkpoint]#, esCallback]\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          #validation_data=(x_val, y_val), \n",
    "          validation_split=0.2,\n",
    "          shuffle = True,\n",
    "          sample_weight = trn_weights,\n",
    "          callbacks=callbacks_list\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(y_val))\n",
    "#print(len(y_val[ [not np.array_equal(i, np.array([0,0,0,0,0,0])) for i in y_val], : ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hist\n",
    "plt.figure(1)  \n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implement Trained Model\n",
    "from models.TrainedModel import TrainedModel\n",
    "import modelDB\n",
    "\n",
    "class TrainedModelCNNEmbeddingMultiClass(TrainedModel):\n",
    "    def __init__(self, md = None):\n",
    "        super(TrainedModelCNNEmbeddingMultiClass, self).__init__(md)\n",
    "    \n",
    "    def predict(self, test, **kwargs):\n",
    "        print (\"Predict using Model: \")\n",
    "        print( type(self.md).__name__ )\n",
    "            \n",
    "        res = self.md.predict(test, batch_size = 1024)\n",
    "        #important to keep the order as required by the submission file\n",
    "        testid = kwargs['testid'] \n",
    "        #print (testid)\n",
    "        dfres = pd.DataFrame(res,columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "        dfres['id'] = testid\n",
    "        \n",
    "        #print(dfres.shape)\n",
    "        #assert(dfres.shape[0] == test.shape[0])\n",
    "        \n",
    "        #reshape to submission file format\n",
    "        dfres = dfres[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "        \n",
    "        return dfres\n",
    "    \n",
    "    def _save(self, mDB, nameKey, modelpath, **kwargs):\n",
    "        '''\n",
    "        save model into model data base\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        print(\"Saving model\")\n",
    "        import datetime\n",
    "        \n",
    "        #save a dict (topic => ('modelname', 'weightname')) into db\n",
    "        model_saved_toDB = {}\n",
    "        \n",
    "        submd = self.md\n",
    "        #this is the file we will save model to\n",
    "        mdname = os.path.join(modelpath, nameKey +'_' + '.sav' )\n",
    "        mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "\n",
    "        #this is the file we will save weights to\n",
    "        weightName = os.path.join(modelpath, nameKey + '_' + '_weights.h5' )\n",
    "        weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "\n",
    "        #convert md to json and save to file\n",
    "        print(\" Saving model {}\".format(mdname) )\n",
    "        model_json = submd.to_json()\n",
    "        with open(mdnameAbs, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        print(\" Saving weights {}\".format(weightName) )\n",
    "        submd.save_weights(weightNameAbs)\n",
    "    \n",
    "        model_saved_toDB['MultiClassModel'] = (mdname, weightName)\n",
    "            \n",
    "        \n",
    "        #db schema\n",
    "        # 'modelName', type {rnn, cnn, rf}, date, model\n",
    "        print( \"Info: custom saving options\" )\n",
    "        for i in kwargs:\n",
    "            print(\"{} - {}\".format(i , kwargs[i]))\n",
    "            \n",
    "        import json\n",
    "        newRow = pd.DataFrame({\n",
    "            'modelName': [nameKey],\n",
    "            'type': kwargs['modelType'],\n",
    "            'subType': kwargs['modelSubType'],\n",
    "            'date': str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "            'model' : json.dumps(model_saved_toDB)\n",
    "        }\n",
    "        )\n",
    "        \n",
    "        #add a new row \n",
    "        #mDB = pd.concat([mDB, newRow])\n",
    "        mDB = mDB.append(newRow, ignore_index = True)\n",
    "        display(mDB)\n",
    "\n",
    "        return mDB\n",
    "    \n",
    "    def load(self, mDB, nameKey, modelpath):\n",
    "        '''\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        from keras.models import model_from_json\n",
    "        import json\n",
    "        \n",
    "        if (mDB.empty) or mDB[ mDB['modelName'] == nameKey ].empty:\n",
    "            raise VaueError(\"Model name does not exist\")\n",
    "        print(\"loadModel\")\n",
    "        #the saved model is of format: dict (topic => ('modelname', 'weightname')) into db\n",
    "        assert(len(mDB[mDB['modelName'] == nameKey]) == 1)\n",
    "        saved_model_inDB = json.loads(mDB[mDB['modelName'] == nameKey].iloc[0]['model'])\n",
    "        \n",
    "        print(\"saved json string representing the model is {}\".format(saved_model_inDB))\n",
    "\n",
    "        key = 'MultiClassModel'\n",
    "        mdPointer = saved_model_inDB[key]\n",
    "        print(\" load model\")\n",
    "        mdname = mdPointer[0]\n",
    "        mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "        print(\"  model file in {}\".format(mdnameAbs))\n",
    "\n",
    "        #this is the file we will save weights to\n",
    "        weightName = mdPointer[1]\n",
    "        weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "        print(\"  weight file in {}\".format(weightNameAbs))\n",
    "            \n",
    "        with open(mdnameAbs, 'r') as json_file:\n",
    "            tmpModel_json = json_file.read()\n",
    "        tmpModel = model_from_json(tmpModel_json)\n",
    "            #load weights\n",
    "        tmpModel.load_weights(weightNameAbs)\n",
    "            \n",
    "            #assign the model into loaded model dict\n",
    "        loaded_model = tmpModel\n",
    "        \n",
    "        assert(self.md is None)\n",
    "        self.setModel(loaded_model)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utest test predict\n",
    "if ( modelType == 'LSTMWithKeyWordTermFreq' )  or (modelType == \"LSTMWithtfIdf\"):\n",
    "    utestInput = [trainOrig[0:1000, :], train2ndOrig[0:1000,:]]\n",
    "else:\n",
    "    utestInput = trainOrig[0:1000, :]\n",
    "\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "dfres = myCNN1d.predict(utestInput, testid = train['id'])\n",
    "display(dfres.head(20))\n",
    "display(train.head(20)[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']])\n",
    "\n",
    "#Utest test save\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "print(\"current modelDB\")\n",
    "display(mdDB)\n",
    "\n",
    "mdDB = myCNN1d.save(mdDB, 'utestModel', modelpath, modelType = 'CNN1d', modelSubType = 'Embedding_Random')\n",
    "#mdDB.to_json(\"../modelDB/modelMetaDB.json\")\n",
    "\n",
    "#Utest test load\n",
    "loadedCNN = TrainedModelCNNEmbeddingMultiClass().load( mdDB, 'utestModel', modelpath)\n",
    "dfres_loaded = loadedCNN.predict(utestInput, testid = train['id'])\n",
    "assert(dfres_loaded.equals(dfres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#forward pass to inference\n",
    "if secondInput(modelSubTypeList) is not None:\n",
    "    testOrig = [np.array(test['seq_pad'].tolist()), test_2nd_input]\n",
    "else:\n",
    "    testOrig = np.array(test['seq_pad'].tolist())\n",
    "\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "\n",
    "dfres = myCNN1d.predict(testOrig, testid = test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "#print(\"current modelDB\")\n",
    "#display(mdDB)\n",
    "mdDB = myCNN1d.save(mdDB, modelName, modelpath, modelType = modelType, modelSubType = modelSubType)\n",
    "mdDB.to_pickle(\"../modelDB/modelMetaDB.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "dfres.to_csv('../submission/'+modelName+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
