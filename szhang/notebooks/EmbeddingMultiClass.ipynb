{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "## Math and dataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, confusion_matrix\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set len  159571\n",
      "test set len  153164\n",
      "clean samples 143346\n",
      "toxic samples 16225\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene',  'threat', 'insult', 'identity_hate']\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train['dirtyness'] = train.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "test['dirtyness'] = test.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "print(\"train set len \", len(train) )\n",
    "print(\"test set len \", len(test) )\n",
    "print(\"clean samples\", len(train[train['dirtyness'] == 0]))\n",
    "print(\"toxic samples\", len(train[train['dirtyness'] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "modelType = 'LSTM'\n",
    "modelSubTypeList = ['MultiClass_Embedding_Random']\n",
    "modelSubType = '_'.join(modelSubTypeList)\n",
    "modelName = modelType + '_' + modelSubType + '_arch1'\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_dims = 50\n",
    "filters = 120\n",
    "hidden_dims = 50\n",
    "kernel_size = 3\n",
    "\n",
    "#preprocessing\n",
    "raw_text = np.hstack([train.comment_text.str.lower(), test.comment_text.str.lower()])\n",
    "tok_raw = Tokenizer(num_words=max_features)\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "train[\"seq\"] = tok_raw.texts_to_sequences(train.comment_text.str.lower())\n",
    "test[\"seq\"] = tok_raw.texts_to_sequences(test.comment_text.str.lower())\n",
    "\n",
    "#sequence.pad_sequences(train['comment_text'].values, maxlen=maxlen)\n",
    "#test['comment_text'] = sequence.pad_sequences(test['comment_text'], maxlen=maxlen)\n",
    "#print('x_train shape:', train['comment_text'].values.shape)\n",
    "#print('x_test shape:', test['comment_text'].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad\n",
    "#train[\"seq\"].apply(lambda x: len(x)).describe()\n",
    "train[\"seq_pad\"] = train[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n",
    "test[\"seq_pad\"] = test[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fe , we will use resample to balance the labels\n",
    "trn_2nd_input = None\n",
    "test_2nd_input = None\n",
    "\n",
    "if modelType == 'LSTMWithKeyWordTermFreq':\n",
    "    from models.FeatureExtraction import FeatureExtraction\n",
    "\n",
    "    fe = FeatureExtraction()\n",
    "\n",
    "    keyword_dir = '../../ZhiHaoSun/'\n",
    "    keyfiles = [\n",
    "                keyword_dir + 'toxic_words.txt',\n",
    "                keyword_dir + 'identity_hate_words.txt',\n",
    "                keyword_dir + 'insult_words.txt',\n",
    "                keyword_dir + 'obscene_words.txt',\n",
    "                keyword_dir + 'threat_words.txt',\n",
    "                keyword_dir + 'identity_hate_words.txt',\n",
    "                ]\n",
    "\n",
    "    term_doc = fe.tfKeyWordEnsemble(\n",
    "                pd.concat([train, test]), n_feature = 80000, vocabfile = keyfiles,\n",
    "                COMMENT = 'comment_text'\n",
    "                )\n",
    "\n",
    "    trn_term_doc = term_doc.tocsr()[0:len(train), :]\n",
    "    test_term_doc = term_doc.tocsr()[len(train)::, :]\n",
    "    trn_2nd_input = trn_term_doc\n",
    "    test_2nd_input = test_term_doc\n",
    "    \n",
    "    aux_input_dim = trn_2nd_input.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_23 (Embedding)     (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 100, 100)          40400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_20 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 1,045,756\n",
      "Trainable params: 1,045,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "def buildModel(modelType):\n",
    "    if modelType == 'CNN1d':\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        model.add(Embedding(max_features,\n",
    "                            embedding_dims,\n",
    "                            input_length=maxlen, \n",
    "                            #embeddings_regularizer = keras.regularizers.l1(0.01)\n",
    "                            ))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # we add a Convolution1D, which will learn filters\n",
    "        # word group filters of size filter_length:\n",
    "        model.add(Conv1D(filters,\n",
    "                         kernel_size,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(BatchNormalization())\n",
    "        # we use max pooling:\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "\n",
    "        # We add a vanilla hidden layer:\n",
    "        model.add(Dense(hidden_dims))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "        model.add(Dense(6))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        # define metrics and compile model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    elif modelType == 'LSTM':\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embedding_dims)(inp)\n",
    "        x = Bidirectional(LSTM(hidden_dims, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        \n",
    "        x = Dense(hidden_dims, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    elif modelType == 'LSTMWithKeyWordTermFreq':\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embedding_dims)(inp)\n",
    "        x = Bidirectional(LSTM(hidden_dims, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "                \n",
    "        auxiliary_input = Input(shape=(aux_input_dim,), name='aux_input')\n",
    "        x = keras.layers.concatenate([x, auxiliary_input])\n",
    "\n",
    "        x = Dense(hidden_dims, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=[inp, auxiliary_input], outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"undefined model type\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(modelType)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load fe , we will use resample to balance the labels\n",
    "from models.FeatureExtraction import FeatureExtraction\n",
    "\n",
    "fe = FeatureExtraction()\n",
    "\n",
    "trainOrig = np.array(train['seq_pad'].tolist())\n",
    "assert(trainOrig.shape == (len(train), maxlen))\n",
    "\n",
    "train2ndOrig = trn_2nd_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9741\n",
      "Epoch 00001: val_loss improved from -inf to 0.05128, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 287s 2ms/step - loss: 0.0813 - acc: 0.9741 - val_loss: 0.0513 - val_acc: 0.9817\n",
      "Epoch 2/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9824\n",
      "Epoch 00002: val_loss did not improve\n",
      "127656/127656 [==============================] - 281s 2ms/step - loss: 0.0479 - acc: 0.9824 - val_loss: 0.0491 - val_acc: 0.9821\n",
      "Epoch 3/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9837\n",
      "Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 349s 3ms/step - loss: 0.0437 - acc: 0.9837 - val_loss: 0.0504 - val_acc: 0.9821\n",
      "Epoch 4/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9844\n",
      "Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 282s 2ms/step - loss: 0.0406 - acc: 0.9844 - val_loss: 0.0503 - val_acc: 0.9821\n"
     ]
    }
   ],
   "source": [
    "#define F1 metric\n",
    "import keras\n",
    "#fit\n",
    "reSample = False\n",
    "\n",
    "# train test split\n",
    "if reSample == True:\n",
    "    assert(train2ndOrig is None)\n",
    "    trn_re, label_re = fe.reSample( scipy.sparse.csr_matrix(trainOrig) , y = train[label_cols].values)\n",
    "else:\n",
    "    trn_re, label_re = scipy.sparse.csr_matrix(trainOrig), train[label_cols].values\n",
    "    train2nd_re = train2ndOrig\n",
    "    \n",
    "#x_train, x_val, y_train, y_val = train_test_split(trn_re, label_re, train2ndOrig, test_size=0.33, random_state=42)\n",
    "y_train = label_re\n",
    "if modelType == 'LSTMWithKeyWordTermFreq':\n",
    "    x_train = [trn_re, x_train_2nd]\n",
    "else:\n",
    "    x_train = trn_re\n",
    "\n",
    "\n",
    "# add check point\n",
    "filepath= type(model).__name__ + \"_weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "esCallback = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "callbacks_list = [checkpoint, esCallback]\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          #validation_data=(x_val, y_val), \n",
    "          validation_split=0.2,\n",
    "          callbacks=callbacks_list\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(y_val))\n",
    "#print(len(y_val[ [ not np.array_equal(i, np.array([0,0,0,0,0,0])) for i in y_val], : ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x120846c10>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6ceee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = hist\n",
    "plt.figure(1)  \n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implement Trained Model\n",
    "from models.TrainedModel import TrainedModel\n",
    "import modelDB\n",
    "\n",
    "class TrainedModelCNNEmbeddingMultiClass(TrainedModel):\n",
    "    def __init__(self, md = None):\n",
    "        super(TrainedModelCNNEmbeddingMultiClass, self).__init__(md)\n",
    "    \n",
    "    def predict(self, test, **kwargs):\n",
    "        print (\"Predict using Model: \")\n",
    "        print( type(self.md).__name__ )\n",
    "            \n",
    "        res = self.md.predict(test, batch_size = 1024)\n",
    "        #important to keep the order as required by the submission file\n",
    "        testid = kwargs['testid'] \n",
    "        #print (testid)\n",
    "        dfres = pd.DataFrame(res,columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "        dfres['id'] = testid\n",
    "        \n",
    "        #print(dfres.shape)\n",
    "        #assert(dfres.shape[0] == test.shape[0])\n",
    "        \n",
    "        #reshape to submission file format\n",
    "        dfres = dfres[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "        \n",
    "        return dfres\n",
    "    \n",
    "    def _save(self, mDB, nameKey, modelpath, **kwargs):\n",
    "        '''\n",
    "        save model into model data base\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        print(\"Saving model\")\n",
    "        import datetime\n",
    "        \n",
    "        #save a dict (topic => ('modelname', 'weightname')) into db\n",
    "        model_saved_toDB = {}\n",
    "        \n",
    "        submd = self.md\n",
    "        #this is the file we will save model to\n",
    "        mdname = os.path.join(modelpath, nameKey +'_' + '.sav' )\n",
    "        mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "\n",
    "        #this is the file we will save weights to\n",
    "        weightName = os.path.join(modelpath, nameKey + '_' + '_weights.h5' )\n",
    "        weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "\n",
    "        #convert md to json and save to file\n",
    "        print(\" Saving model {}\".format(mdname) )\n",
    "        model_json = submd.to_json()\n",
    "        with open(mdnameAbs, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        print(\" Saving weights {}\".format(weightName) )\n",
    "        submd.save_weights(weightNameAbs)\n",
    "    \n",
    "        model_saved_toDB['MultiClassModel'] = (mdname, weightName)\n",
    "            \n",
    "        \n",
    "        #db schema\n",
    "        # 'modelName', type {rnn, cnn, rf}, date, model\n",
    "        print( \"Info: custom saving options\" )\n",
    "        for i in kwargs:\n",
    "            print(\"{} - {}\".format(i , kwargs[i]))\n",
    "            \n",
    "        import json\n",
    "        newRow = pd.DataFrame({\n",
    "            'modelName': [nameKey],\n",
    "            'type': kwargs['modelType'],\n",
    "            'subType': kwargs['modelSubType'],\n",
    "            'date': str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "            'model' : json.dumps(model_saved_toDB)\n",
    "        }\n",
    "        )\n",
    "        \n",
    "        #add a new row \n",
    "        #mDB = pd.concat([mDB, newRow])\n",
    "        mDB = mDB.append(newRow, ignore_index = True)\n",
    "        display(mDB)\n",
    "\n",
    "        return mDB\n",
    "    \n",
    "    def load(self, mDB, nameKey, modelpath):\n",
    "        '''\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        from keras.models import model_from_json\n",
    "        import json\n",
    "        \n",
    "        if (mDB.empty) or mDB[ mDB['modelName'] == nameKey ].empty:\n",
    "            raise VaueError(\"Model name does not exist\")\n",
    "        print(\"loadModel\")\n",
    "        #the saved model is of format: dict (topic => ('modelname', 'weightname')) into db\n",
    "        assert(len(mDB[mDB['modelName'] == nameKey]) == 1)\n",
    "        saved_model_inDB = json.loads(mDB[mDB['modelName'] == nameKey].iloc[0]['model'])\n",
    "        \n",
    "        print(\"saved json string representing the model is {}\".format(saved_model_inDB))\n",
    "\n",
    "        key = 'MultiClassModel'\n",
    "        mdPointer = saved_model_inDB[key]\n",
    "        print(\" load model\")\n",
    "        mdname = mdPointer[0]\n",
    "        mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "        print(\"  model file in {}\".format(mdnameAbs))\n",
    "\n",
    "        #this is the file we will save weights to\n",
    "        weightName = mdPointer[1]\n",
    "        weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "        print(\"  weight file in {}\".format(weightNameAbs))\n",
    "            \n",
    "        with open(mdnameAbs, 'r') as json_file:\n",
    "            tmpModel_json = json_file.read()\n",
    "        tmpModel = model_from_json(tmpModel_json)\n",
    "            #load weights\n",
    "        tmpModel.load_weights(weightNameAbs)\n",
    "            \n",
    "            #assign the model into loaded model dict\n",
    "        loaded_model = tmpModel\n",
    "        \n",
    "        assert(self.md is None)\n",
    "        self.setModel(loaded_model)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utest test predict\n",
    "if modelType == 'LSTMWithKeyWordTermFreq':\n",
    "    utestInput = [trainOrig[0:1000, :], train2ndOrig[0:1000,:]]\n",
    "else:\n",
    "    utestInput = trainOrig[0:1000, :]\n",
    "\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "dfres = myCNN1d.predict(utestInput, testid = train['id'])\n",
    "display(dfres.head(20))\n",
    "display(train.head(20)[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']])\n",
    "\n",
    "#Utest test save\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "print(\"current modelDB\")\n",
    "display(mdDB)\n",
    "\n",
    "mdDB = myCNN1d.save(mdDB, 'utestModel', modelpath, modelType = 'CNN1d', modelSubType = 'Embedding_Random')\n",
    "#mdDB.to_json(\"../modelDB/modelMetaDB.json\")\n",
    "\n",
    "#Utest test load\n",
    "loadedCNN = TrainedModelCNNEmbeddingMultiClass().load( mdDB, 'utestModel', modelpath)\n",
    "dfres_loaded = loadedCNN.predict(utestInput, testid = train['id'])\n",
    "assert(dfres_loaded.equals(dfres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict using Model: \n",
      "Model\n"
     ]
    }
   ],
   "source": [
    "#forward pass to inference\n",
    "if modelType == 'LSTMWithKeyWordTermFreq':\n",
    "    testOrig = [np.array(test['seq_pad'].tolist()), test_2nd_input]\n",
    "else:\n",
    "    testOrig = np.array(test['seq_pad'].tolist())\n",
    "\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "\n",
    "dfres = myCNN1d.predict(testOrig, testid = test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: found identical nameKey, overwrite: [y/n]?y\n",
      "Saving model\n",
      " Saving model cnn/LSTM_MultiClass_Embedding_Random_arch1_.sav\n",
      " Saving weights cnn/LSTM_MultiClass_Embedding_Random_arch1__weights.h5\n",
      "Info: custom saving options\n",
      "modelSubType - MultiClass_Embedding_Random\n",
      "modelType - LSTM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model</th>\n",
       "      <th>modelName</th>\n",
       "      <th>subType</th>\n",
       "      <th>type</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>cnn/utestModel.sav</td>\n",
       "      <td>utestModel</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>testType</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass_arch1</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>svc/SVC_tfidf_1.sav</td>\n",
       "      <td>SVC_tfidf_1</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>svc/SVC_keywordTermFreq_1.sav</td>\n",
       "      <td>SVC_keywordTermFreq_1</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass_arch2</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/CNN1d_MultiClass_Emb...</td>\n",
       "      <td>CNN1d_MultiClass_Embedding_Random_arch1</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTMWithKeyWordTermF...</td>\n",
       "      <td>LSTMWithKeyWordTermFreq_MultiClass_Embedding_R...</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTMWithKeyWordTermFreq</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...</td>\n",
       "      <td>LSTM_MultiClass_Embedding_Random_arch1</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              model  \\\n",
       "0  1970-01-01 00:00:00  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "1  1970-01-01 00:00:00                                 cnn/utestModel.sav   \n",
       "2           2018-02-13  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "3           2018-02-13                                svc/SVC_tfidf_1.sav   \n",
       "4           2018-02-13                      svc/SVC_keywordTermFreq_1.sav   \n",
       "5           2018-02-14  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "6           2018-02-14  {\"MultiClassModel\": [\"cnn/CNN1d_MultiClass_Emb...   \n",
       "7           2018-02-17  {\"MultiClassModel\": [\"cnn/LSTMWithKeyWordTermF...   \n",
       "8           2018-02-17  {\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...   \n",
       "\n",
       "                                           modelName  \\\n",
       "0                 CNN1d-Embedding_Random-OneperClass   \n",
       "1                                         utestModel   \n",
       "2           CNN1d-Embedding_Random-OneperClass_arch1   \n",
       "3                                        SVC_tfidf_1   \n",
       "4                              SVC_keywordTermFreq_1   \n",
       "5           CNN1d-Embedding_Random-OneperClass_arch2   \n",
       "6            CNN1d_MultiClass_Embedding_Random_arch1   \n",
       "7  LSTMWithKeyWordTermFreq_MultiClass_Embedding_R...   \n",
       "8             LSTM_MultiClass_Embedding_Random_arch1   \n",
       "\n",
       "                       subType                     type  weights  \n",
       "0             Embedding_Random                    CNN1d      NaN  \n",
       "1                        TfIdf                 testType      NaN  \n",
       "2             Embedding_Random                    CNN1d      NaN  \n",
       "3                        TfIdf                      SVC      NaN  \n",
       "4                        TfIdf                      SVC      NaN  \n",
       "5             Embedding_Random                    CNN1d      NaN  \n",
       "6  MultiClass_Embedding_Random                    CNN1d      NaN  \n",
       "7  MultiClass_Embedding_Random  LSTMWithKeyWordTermFreq      NaN  \n",
       "8  MultiClass_Embedding_Random                     LSTM      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "#print(\"current modelDB\")\n",
    "#display(mdDB)\n",
    "mdDB = myCNN1d.save(mdDB, modelName, modelpath, modelType = modelType, modelSubType = modelSubType)\n",
    "mdDB.to_pickle(\"../modelDB/modelMetaDB.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "dfres.to_csv('../submission/'+modelName+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
