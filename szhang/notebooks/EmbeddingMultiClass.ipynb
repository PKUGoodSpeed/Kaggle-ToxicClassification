{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "## Math and dataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ML\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, confusion_matrix\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set len  159571\n",
      "test set len  153164\n",
      "clean samples 143346\n",
      "toxic samples 16225\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene',  'threat', 'insult', 'identity_hate']\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train['dirtyness'] = train.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "test['dirtyness'] = test.apply(lambda x: x.iloc[2::].sum(), axis = 1)\n",
    "\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "print(\"train set len \", len(train) )\n",
    "print(\"test set len \", len(test) )\n",
    "print(\"clean samples\", len(train[train['dirtyness'] == 0]))\n",
    "print(\"toxic samples\", len(train[train['dirtyness'] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "modelType = 'LSTMWithtfIdf'\n",
    "modelSubTypeList = ['MultiClass_Embedding_Random']\n",
    "modelSubType = '_'.join(modelSubTypeList)\n",
    "modelName = modelType + '_' + modelSubType + '_arch1'\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "max_features = 40000\n",
    "maxlen = 100*1\n",
    "embedding_dims = 50\n",
    "filters = 120\n",
    "hidden_dims = 50\n",
    "kernel_size = 3\n",
    "\n",
    "#preprocessing\n",
    "raw_text = np.hstack([train.comment_text.str.lower(), test.comment_text.str.lower()])\n",
    "tok_raw = Tokenizer(num_words=max_features)\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "train[\"seq\"] = tok_raw.texts_to_sequences(train.comment_text.str.lower())\n",
    "test[\"seq\"] = tok_raw.texts_to_sequences(test.comment_text.str.lower())\n",
    "\n",
    "#sequence.pad_sequences(train['comment_text'].values, maxlen=maxlen)\n",
    "#test['comment_text'] = sequence.pad_sequences(test['comment_text'], maxlen=maxlen)\n",
    "#print('x_train shape:', train['comment_text'].values.shape)\n",
    "#print('x_test shape:', test['comment_text'].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad\n",
    "#train[\"seq\"].apply(lambda x: len(x)).describe()\n",
    "train[\"seq_pad\"] = train[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n",
    "test[\"seq_pad\"] = test[\"seq\"].apply(lambda x, maxlen: sequence.pad_sequences([x], maxlen=maxlen)[0], args = [maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "# load fe , we will use resample to balance the labels\n",
    "trn_2nd_input = None\n",
    "test_2nd_input = None\n",
    "\n",
    "if modelType == 'LSTMWithKeyWordTermFreq':\n",
    "    from models.FeatureExtraction import FeatureExtraction\n",
    "\n",
    "    fe = FeatureExtraction()\n",
    "\n",
    "    keyword_dir = '../../ZhiHaoSun/'\n",
    "    keyfiles = [\n",
    "                keyword_dir + 'toxic_words.txt',\n",
    "                keyword_dir + 'identity_hate_words.txt',\n",
    "                keyword_dir + 'insult_words.txt',\n",
    "                keyword_dir + 'obscene_words.txt',\n",
    "                keyword_dir + 'threat_words.txt',\n",
    "                keyword_dir + 'identity_hate_words.txt',\n",
    "                ]\n",
    "\n",
    "    term_doc = fe.tfKeyWordEnsemble(\n",
    "                pd.concat([train, test]), n_feature = 80000, vocabfile = keyfiles,\n",
    "                COMMENT = 'comment_text'\n",
    "                )\n",
    "\n",
    "    trn_term_doc = term_doc.tocsr()[0:len(train), :]\n",
    "    test_term_doc = term_doc.tocsr()[len(train)::, :]\n",
    "    trn_2nd_input = trn_term_doc\n",
    "    test_2nd_input = test_term_doc\n",
    "    \n",
    "    aux_input_dim = trn_2nd_input.shape[1]\n",
    "elif modelType == \"LSTMWithtfIdf\":\n",
    "    term_doc = fe.tfIdf(pd.concat([train, test]), 'comment_text')\n",
    "    trn_term_doc = term_doc.tocsr()[0:len(train), :]\n",
    "    test_term_doc = term_doc.tocsr()[len(train)::, :]\n",
    "    trn_2nd_input = trn_term_doc\n",
    "    test_2nd_input = test_term_doc\n",
    "    \n",
    "    aux_input_dim = trn_2nd_input.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_33 (Embedding)        (None, 100, 50)      2000000     input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 689099)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 100, 100)     40400       embedding_33[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 3)            2067300     aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 100)          0           bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 3)            0           dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 103)          0           global_max_pooling1d_32[0][0]    \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 50)           5200        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 50)           0           dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 6)            306         dropout_43[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,113,206\n",
      "Trainable params: 4,113,206\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "def buildModel(modelType):\n",
    "    if modelType == 'CNN1d':\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        model.add(Embedding(max_features,\n",
    "                            embedding_dims,\n",
    "                            input_length=maxlen, \n",
    "                            #embeddings_regularizer = keras.regularizers.l1(0.01)\n",
    "                            ))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # we add a Convolution1D, which will learn filters\n",
    "        # word group filters of size filter_length:\n",
    "        model.add(Conv1D(filters,\n",
    "                         kernel_size,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(BatchNormalization())\n",
    "        # we use max pooling:\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "\n",
    "        # We add a vanilla hidden layer:\n",
    "        model.add(Dense(hidden_dims))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "        model.add(Dense(6))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        # define metrics and compile model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    elif modelType == 'LSTM':\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embedding_dims)(inp)\n",
    "        x = Bidirectional(LSTM(hidden_dims, return_sequences=True, dropout=0.6, recurrent_dropout=0.6))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        \n",
    "        x = Dense(hidden_dims, activation=\"relu\")(x)\n",
    "        x = Dropout(0.7)(x)\n",
    "        x = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    elif (modelType == 'LSTMWithKeyWordTermFreq') or (modelType == \"LSTMWithtfIdf\"):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        x = Embedding(max_features, embedding_dims)(inp)\n",
    "        x = Bidirectional(LSTM(hidden_dims, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "                \n",
    "        auxiliary_input = Input(shape=(aux_input_dim,), name='aux_input')\n",
    "        y = Dense(3, activation=\"relu\")(auxiliary_input)\n",
    "        y = Dropout(0.6)(y)\n",
    "\n",
    "        x = keras.layers.concatenate([x, y])\n",
    "\n",
    "        x = Dense(hidden_dims, activation=\"relu\")(x)\n",
    "        x = Dropout(0.6)(x)\n",
    "        x = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=[inp, auxiliary_input], outputs=x)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"undefined model type\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(modelType)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load fe , we will use resample to balance the labels\n",
    "from models.FeatureExtraction import FeatureExtraction\n",
    "\n",
    "fe = FeatureExtraction()\n",
    "\n",
    "trainOrig = np.array(train['seq_pad'].tolist())\n",
    "assert(trainOrig.shape == (len(train), maxlen))\n",
    "\n",
    "train2ndOrig = trn_2nd_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9719\n",
      "Epoch 00001: val_loss improved from -inf to 0.05067, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 1163s 9ms/step - loss: 0.0899 - acc: 0.9719 - val_loss: 0.0507 - val_acc: 0.9814\n",
      "Epoch 2/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9814\n",
      "Epoch 00002: val_loss improved from 0.05067 to 0.05191, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 1150s 9ms/step - loss: 0.0521 - acc: 0.9814 - val_loss: 0.0519 - val_acc: 0.9815\n",
      "Epoch 3/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9827\n",
      "Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 1089s 9ms/step - loss: 0.0471 - acc: 0.9827 - val_loss: 0.0512 - val_acc: 0.9821\n",
      "Epoch 4/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9836\n",
      "Epoch 00004: val_loss improved from 0.05191 to 0.05480, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 1081s 8ms/step - loss: 0.0435 - acc: 0.9836 - val_loss: 0.0548 - val_acc: 0.9822\n",
      "Epoch 5/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9844\n",
      "Epoch 00005: val_loss improved from 0.05480 to 0.05574, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 1096s 9ms/step - loss: 0.0407 - acc: 0.9844 - val_loss: 0.0557 - val_acc: 0.9821\n",
      "Epoch 6/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9850\n",
      "Epoch 00006: val_loss improved from 0.05574 to 0.05889, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 1032s 8ms/step - loss: 0.0384 - acc: 0.9850 - val_loss: 0.0589 - val_acc: 0.9820\n",
      "Epoch 7/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9858\n",
      "Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 1044s 8ms/step - loss: 0.0364 - acc: 0.9858 - val_loss: 0.0568 - val_acc: 0.9813\n",
      "Epoch 8/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9864\n",
      "Epoch 00008: val_loss improved from 0.05889 to 0.06398, saving model to Model_weights_best.hdf5\n",
      "127656/127656 [==============================] - 1077s 8ms/step - loss: 0.0349 - acc: 0.9864 - val_loss: 0.0640 - val_acc: 0.9816\n"
     ]
    }
   ],
   "source": [
    "#define F1 metric\n",
    "import keras\n",
    "#fit\n",
    "reSample = False\n",
    "\n",
    "# train test split\n",
    "if reSample == True:\n",
    "    assert(train2ndOrig is None)\n",
    "    trn_re, label_re = fe.reSample( scipy.sparse.csr_matrix(trainOrig) , y = train[label_cols].values)\n",
    "else:\n",
    "    trn_re, label_re = scipy.sparse.csr_matrix(trainOrig), train[label_cols].values\n",
    "    train2nd_re = train2ndOrig\n",
    "    \n",
    "#x_train, x_val, y_train, y_val = train_test_split(trn_re, label_re, train2ndOrig, test_size=0.33, random_state=42)\n",
    "y_train = label_re\n",
    "if (modelType == 'LSTMWithKeyWordTermFreq') or (modelType == \"LSTMWithtfIdf\"):\n",
    "    x_train = [trn_re, train2nd_re]\n",
    "else:\n",
    "    x_train = trn_re\n",
    "\n",
    "\n",
    "# add check point\n",
    "filepath= type(model).__name__ + \"_weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "esCallback = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "callbacks_list = [checkpoint]#, esCallback]\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          #validation_data=(x_val, y_val), \n",
    "          validation_split=0.2,\n",
    "          callbacks=callbacks_list\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(y_val))\n",
    "#print(len(y_val[ [ not np.array_equal(i, np.array([0,0,0,0,0,0])) for i in y_val], : ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b12dc410>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19ce4c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = hist\n",
    "plt.figure(1)  \n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implement Trained Model\n",
    "from models.TrainedModel import TrainedModel\n",
    "import modelDB\n",
    "\n",
    "class TrainedModelCNNEmbeddingMultiClass(TrainedModel):\n",
    "    def __init__(self, md = None):\n",
    "        super(TrainedModelCNNEmbeddingMultiClass, self).__init__(md)\n",
    "    \n",
    "    def predict(self, test, **kwargs):\n",
    "        print (\"Predict using Model: \")\n",
    "        print( type(self.md).__name__ )\n",
    "            \n",
    "        res = self.md.predict(test, batch_size = 1024)\n",
    "        #important to keep the order as required by the submission file\n",
    "        testid = kwargs['testid'] \n",
    "        #print (testid)\n",
    "        dfres = pd.DataFrame(res,columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "        dfres['id'] = testid\n",
    "        \n",
    "        #print(dfres.shape)\n",
    "        #assert(dfres.shape[0] == test.shape[0])\n",
    "        \n",
    "        #reshape to submission file format\n",
    "        dfres = dfres[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "        \n",
    "        return dfres\n",
    "    \n",
    "    def _save(self, mDB, nameKey, modelpath, **kwargs):\n",
    "        '''\n",
    "        save model into model data base\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        print(\"Saving model\")\n",
    "        import datetime\n",
    "        \n",
    "        #save a dict (topic => ('modelname', 'weightname')) into db\n",
    "        model_saved_toDB = {}\n",
    "        \n",
    "        submd = self.md\n",
    "        #this is the file we will save model to\n",
    "        mdname = os.path.join(modelpath, nameKey +'_' + '.sav' )\n",
    "        mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "\n",
    "        #this is the file we will save weights to\n",
    "        weightName = os.path.join(modelpath, nameKey + '_' + '_weights.h5' )\n",
    "        weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "\n",
    "        #convert md to json and save to file\n",
    "        print(\" Saving model {}\".format(mdname) )\n",
    "        model_json = submd.to_json()\n",
    "        with open(mdnameAbs, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        print(\" Saving weights {}\".format(weightName) )\n",
    "        submd.save_weights(weightNameAbs)\n",
    "    \n",
    "        model_saved_toDB['MultiClassModel'] = (mdname, weightName)\n",
    "            \n",
    "        \n",
    "        #db schema\n",
    "        # 'modelName', type {rnn, cnn, rf}, date, model\n",
    "        print( \"Info: custom saving options\" )\n",
    "        for i in kwargs:\n",
    "            print(\"{} - {}\".format(i , kwargs[i]))\n",
    "            \n",
    "        import json\n",
    "        newRow = pd.DataFrame({\n",
    "            'modelName': [nameKey],\n",
    "            'type': kwargs['modelType'],\n",
    "            'subType': kwargs['modelSubType'],\n",
    "            'date': str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "            'model' : json.dumps(model_saved_toDB)\n",
    "        }\n",
    "        )\n",
    "        \n",
    "        #add a new row \n",
    "        #mDB = pd.concat([mDB, newRow])\n",
    "        mDB = mDB.append(newRow, ignore_index = True)\n",
    "        display(mDB)\n",
    "\n",
    "        return mDB\n",
    "    \n",
    "    def load(self, mDB, nameKey, modelpath):\n",
    "        '''\n",
    "        :mDB: meta data frame storing all models info\n",
    "        :nameKey: unique identifier for each saved model\n",
    "        :modelpath: subdir inside modelDB dir, e.g. if modelDB is /root/modelDB, then modelpath is /cnn\n",
    "        '''\n",
    "        from keras.models import model_from_json\n",
    "        import json\n",
    "        \n",
    "        if (mDB.empty) or mDB[ mDB['modelName'] == nameKey ].empty:\n",
    "            raise VaueError(\"Model name does not exist\")\n",
    "        print(\"loadModel\")\n",
    "        #the saved model is of format: dict (topic => ('modelname', 'weightname')) into db\n",
    "        assert(len(mDB[mDB['modelName'] == nameKey]) == 1)\n",
    "        saved_model_inDB = json.loads(mDB[mDB['modelName'] == nameKey].iloc[0]['model'])\n",
    "        \n",
    "        print(\"saved json string representing the model is {}\".format(saved_model_inDB))\n",
    "\n",
    "        key = 'MultiClassModel'\n",
    "        mdPointer = saved_model_inDB[key]\n",
    "        print(\" load model\")\n",
    "        mdname = mdPointer[0]\n",
    "        mdnameAbs = os.path.join(modelDB.MODEL_DB_ROOT, mdname )\n",
    "        print(\"  model file in {}\".format(mdnameAbs))\n",
    "\n",
    "        #this is the file we will save weights to\n",
    "        weightName = mdPointer[1]\n",
    "        weightNameAbs = os.path.join( modelDB.MODEL_DB_ROOT, weightName )\n",
    "        print(\"  weight file in {}\".format(weightNameAbs))\n",
    "            \n",
    "        with open(mdnameAbs, 'r') as json_file:\n",
    "            tmpModel_json = json_file.read()\n",
    "        tmpModel = model_from_json(tmpModel_json)\n",
    "            #load weights\n",
    "        tmpModel.load_weights(weightNameAbs)\n",
    "            \n",
    "            #assign the model into loaded model dict\n",
    "        loaded_model = tmpModel\n",
    "        \n",
    "        assert(self.md is None)\n",
    "        self.setModel(loaded_model)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utest test predict\n",
    "if ( modelType == 'LSTMWithKeyWordTermFreq' )  or (modelType == \"LSTMWithtfIdf\"):\n",
    "    utestInput = [trainOrig[0:1000, :], train2ndOrig[0:1000,:]]\n",
    "else:\n",
    "    utestInput = trainOrig[0:1000, :]\n",
    "\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "dfres = myCNN1d.predict(utestInput, testid = train['id'])\n",
    "display(dfres.head(20))\n",
    "display(train.head(20)[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']])\n",
    "\n",
    "#Utest test save\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "print(\"current modelDB\")\n",
    "display(mdDB)\n",
    "\n",
    "mdDB = myCNN1d.save(mdDB, 'utestModel', modelpath, modelType = 'CNN1d', modelSubType = 'Embedding_Random')\n",
    "#mdDB.to_json(\"../modelDB/modelMetaDB.json\")\n",
    "\n",
    "#Utest test load\n",
    "loadedCNN = TrainedModelCNNEmbeddingMultiClass().load( mdDB, 'utestModel', modelpath)\n",
    "dfres_loaded = loadedCNN.predict(utestInput, testid = train['id'])\n",
    "assert(dfres_loaded.equals(dfres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict using Model: \n",
      "Model\n"
     ]
    }
   ],
   "source": [
    "#forward pass to inference\n",
    "if ( modelType == 'LSTMWithKeyWordTermFreq' )  or (modelType == \"LSTMWithtfIdf\"):\n",
    "    testOrig = [np.array(test['seq_pad'].tolist()), test_2nd_input]\n",
    "else:\n",
    "    testOrig = np.array(test['seq_pad'].tolist())\n",
    "\n",
    "\n",
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "\n",
    "dfres = myCNN1d.predict(testOrig, testid = test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      " Saving model cnn/LSTMWithtfIdf_MultiClass_Embedding_Random_arch1_.sav\n",
      " Saving weights cnn/LSTMWithtfIdf_MultiClass_Embedding_Random_arch1__weights.h5\n",
      "Info: custom saving options\n",
      "modelSubType - MultiClass_Embedding_Random\n",
      "modelType - LSTMWithtfIdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model</th>\n",
       "      <th>modelName</th>\n",
       "      <th>subType</th>\n",
       "      <th>type</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>cnn/utestModel.sav</td>\n",
       "      <td>utestModel</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>testType</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass_arch1</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>svc/SVC_tfidf_1.sav</td>\n",
       "      <td>SVC_tfidf_1</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>svc/SVC_keywordTermFreq_1.sav</td>\n",
       "      <td>SVC_keywordTermFreq_1</td>\n",
       "      <td>TfIdf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>{\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...</td>\n",
       "      <td>CNN1d-Embedding_Random-OneperClass_arch2</td>\n",
       "      <td>Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/CNN1d_MultiClass_Emb...</td>\n",
       "      <td>CNN1d_MultiClass_Embedding_Random_arch1</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>CNN1d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTMWithKeyWordTermF...</td>\n",
       "      <td>LSTMWithKeyWordTermFreq_MultiClass_Embedding_R...</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTMWithKeyWordTermFreq</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...</td>\n",
       "      <td>LSTM_MultiClass_Embedding_Random_arch1</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...</td>\n",
       "      <td>LSTM_MultiClass_Embedding_Random_arch2</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...</td>\n",
       "      <td>LSTM_MultiClass_Embedding_Random_arch3</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...</td>\n",
       "      <td>LSTM_MultiClass_Embedding_Random_arch4</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...</td>\n",
       "      <td>LSTM_MultiClass_Embedding_Random_arch5</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTMWithKeyWordTermF...</td>\n",
       "      <td>LSTMWithKeyWordTermFreq_MultiClass_Embedding_R...</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTMWithKeyWordTermFreq</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>{\"MultiClassModel\": [\"cnn/LSTMWithtfIdf_MultiC...</td>\n",
       "      <td>LSTMWithtfIdf_MultiClass_Embedding_Random_arch1</td>\n",
       "      <td>MultiClass_Embedding_Random</td>\n",
       "      <td>LSTMWithtfIdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                              model  \\\n",
       "0   1970-01-01 00:00:00  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "1   1970-01-01 00:00:00                                 cnn/utestModel.sav   \n",
       "2            2018-02-13  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "3            2018-02-13                                svc/SVC_tfidf_1.sav   \n",
       "4            2018-02-13                      svc/SVC_keywordTermFreq_1.sav   \n",
       "5            2018-02-14  {\"severe_toxic\": [\"cnn/CNN1d-Embedding_Random-...   \n",
       "6            2018-02-14  {\"MultiClassModel\": [\"cnn/CNN1d_MultiClass_Emb...   \n",
       "7            2018-02-17  {\"MultiClassModel\": [\"cnn/LSTMWithKeyWordTermF...   \n",
       "8            2018-02-17  {\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...   \n",
       "9            2018-02-17  {\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...   \n",
       "10           2018-02-17  {\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...   \n",
       "11           2018-02-17  {\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...   \n",
       "12           2018-02-17  {\"MultiClassModel\": [\"cnn/LSTM_MultiClass_Embe...   \n",
       "13           2018-02-17  {\"MultiClassModel\": [\"cnn/LSTMWithKeyWordTermF...   \n",
       "14           2018-02-18  {\"MultiClassModel\": [\"cnn/LSTMWithtfIdf_MultiC...   \n",
       "\n",
       "                                            modelName  \\\n",
       "0                  CNN1d-Embedding_Random-OneperClass   \n",
       "1                                          utestModel   \n",
       "2            CNN1d-Embedding_Random-OneperClass_arch1   \n",
       "3                                         SVC_tfidf_1   \n",
       "4                               SVC_keywordTermFreq_1   \n",
       "5            CNN1d-Embedding_Random-OneperClass_arch2   \n",
       "6             CNN1d_MultiClass_Embedding_Random_arch1   \n",
       "7   LSTMWithKeyWordTermFreq_MultiClass_Embedding_R...   \n",
       "8              LSTM_MultiClass_Embedding_Random_arch1   \n",
       "9              LSTM_MultiClass_Embedding_Random_arch2   \n",
       "10             LSTM_MultiClass_Embedding_Random_arch3   \n",
       "11             LSTM_MultiClass_Embedding_Random_arch4   \n",
       "12             LSTM_MultiClass_Embedding_Random_arch5   \n",
       "13  LSTMWithKeyWordTermFreq_MultiClass_Embedding_R...   \n",
       "14    LSTMWithtfIdf_MultiClass_Embedding_Random_arch1   \n",
       "\n",
       "                        subType                     type  weights  \n",
       "0              Embedding_Random                    CNN1d      NaN  \n",
       "1                         TfIdf                 testType      NaN  \n",
       "2              Embedding_Random                    CNN1d      NaN  \n",
       "3                         TfIdf                      SVC      NaN  \n",
       "4                         TfIdf                      SVC      NaN  \n",
       "5              Embedding_Random                    CNN1d      NaN  \n",
       "6   MultiClass_Embedding_Random                    CNN1d      NaN  \n",
       "7   MultiClass_Embedding_Random  LSTMWithKeyWordTermFreq      NaN  \n",
       "8   MultiClass_Embedding_Random                     LSTM      NaN  \n",
       "9   MultiClass_Embedding_Random                     LSTM      NaN  \n",
       "10  MultiClass_Embedding_Random                     LSTM      NaN  \n",
       "11  MultiClass_Embedding_Random                     LSTM      NaN  \n",
       "12  MultiClass_Embedding_Random                     LSTM      NaN  \n",
       "13  MultiClass_Embedding_Random  LSTMWithKeyWordTermFreq      NaN  \n",
       "14  MultiClass_Embedding_Random            LSTMWithtfIdf      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myCNN1d = TrainedModelCNNEmbeddingMultiClass(model)\n",
    "\n",
    "mdDB = pd.read_pickle(\"../modelDB/modelMetaDB.pkl\")\n",
    "modelpath = 'cnn/' # NOTE: this is relative to the modelDB path\n",
    "#print(\"current modelDB\")\n",
    "#display(mdDB)\n",
    "mdDB = myCNN1d.save(mdDB, modelName, modelpath, modelType = modelType, modelSubType = modelSubType)\n",
    "mdDB.to_pickle(\"../modelDB/modelMetaDB.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "dfres.to_csv('../submission/'+modelName+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
