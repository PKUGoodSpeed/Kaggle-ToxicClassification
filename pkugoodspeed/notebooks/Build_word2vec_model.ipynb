{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## system\n",
    "import os\n",
    "\n",
    "## Math and dataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "## Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "## string process\n",
    "import string\n",
    "import re\n",
    "\n",
    "## fast text\n",
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traditional Machine Learning\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "## Keras\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras import initializers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../data/pretrained/freebase-vectors-skipgram1000-en.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...      0   \n",
       "1  000103f0d9cfb60f  d aww he matches this background colour i m se...      0   \n",
       "2  000113f07ec002fd  hey man i m really not trying to edit war it s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule is more succesful then you ll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>from rfc the title is fine as it is imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>sources zawe ashton on lapland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  yo bitch ja rule is more succesful then you ll...\n",
       "1  0000247867823ef7           from rfc the title is fine as it is imo \n",
       "2  00013b17ad220c46                    sources zawe ashton on lapland "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train_processed.csv\")\n",
    "test = pd.read_csv(\"../data/test_processed.csv\")\n",
    "display(train[:3])\n",
    "display(test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_toxic_phrases.txt\n",
      "identity_hate_words.txt\n",
      "toxic_phrases.txt\n",
      "severe_toxic_words.txt\n",
      "toxic_words.txt\n",
      "threat_phrases.txt\n",
      "insult_words.txt\n",
      "insult_phrases.txt\n",
      "identity_hate_phrases.txt\n",
      "dictionary_builder.ipynb\n",
      "threat_words.txt\n",
      "obscene_phrases.txt\n",
      "obscene_words.txt\n",
      "35382\n",
      "['shit die whore die ', 'die die die ', 'small penis faggot ', 'shit fuck u rape ', 'shit faggot ', 'dick shit cock fuck ', 'die faggot ', 'huge penis ', 'shit go suck ', 'shit suck ', 'small penis closedmouth ', 'small penis ', 'nerd faggot suck ', 'suck dick faggot ', 'suck faggot assholes ', 'big nigger dick shit ', 'nigger dick shit ', 'nigger shit ', 'shit jforget fuck shut ', 'suck ur penis ', 'fat shit go fuck ', 'shit face fuck leave ', 'shit jforget fuck ', 'shit fuck ', 'shit moron ', 'suck asyou suck asshole ', 'fuck youa die ', 'die fuckhead fuck ', 'anthony bradbury sucks penis ', 'die fuck ']\n"
     ]
    }
   ],
   "source": [
    "extra_sent = []\n",
    "path = '../../ZhiHaoSun/'\n",
    "for f in os.listdir(path):\n",
    "    print f\n",
    "    if f[-3:] == 'txt':\n",
    "        fp = open(path + f, 'r')\n",
    "        if 'phrase' in f:\n",
    "            for line in fp:\n",
    "                extra_sent.append(''.join(line.split('-')[:-1]))\n",
    "        else:\n",
    "            for line in f:\n",
    "                extra_sent.append(' '.join(line.split(' ')[:-1]))\n",
    "        fp.close()\n",
    "print len(extra_sent)\n",
    "print extra_sent[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"1234567890qwertyuiopasdfghjklzxcvbnm \"\n",
    "def precess_sent(sent):\n",
    "    return ''.join([c for c in sent if c in ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit die whore die ', 'die die die ', 'small penis faggot ', 'shit fuck u rape ', 'shit faggot ', 'dick shit cock fuck ', 'die faggot ', 'huge penis ', 'shit go suck ', 'shit suck ']\n"
     ]
    }
   ],
   "source": [
    "pool = Pool(4)\n",
    "es = pool.map(precess_sent, extra_sent)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print es[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit die whore die ', 'die die die ', 'small penis faggot ']\n"
     ]
    }
   ],
   "source": [
    "extra_sent = es\n",
    "print es[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sents(s):\n",
    "    return s.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 712 ms, total: 4.15 s\n",
      "Wall time: 10.3 s\n",
      "CPU times: user 4.18 s, sys: 524 ms, total: 4.7 s\n",
      "Wall time: 10.1 s\n",
      "CPU times: user 68 ms, sys: 8 ms, total: 76 ms\n",
      "Wall time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "pool=Pool(4)\n",
    "%time train_sents = pool.map(get_sents, train.comment_text.str.lower())\n",
    "%time test_sents = pool.map(get_sents, test.comment_text.str.lower())\n",
    "%time key_sents = pool.map(get_sents, extra_sent)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(train_sents+test_sents, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.84303564,  5.505512  ,  2.2178435 ,  0.76723886,  1.9893103 ,\n",
       "       -0.3016648 , -1.20258   ,  2.4116788 , -1.4143705 , -1.0409048 ,\n",
       "        1.3660235 , -1.9697036 ,  1.6473902 , -1.4264855 , -1.0103444 ,\n",
       "        0.551771  ,  0.5629895 ,  1.226251  , -2.647517  ,  2.5511672 ,\n",
       "       -0.17387967, -1.9184326 , -1.3316419 ,  0.24120224, -0.9159229 ,\n",
       "        0.74459535, -1.3509506 ,  0.13880473,  0.02293108,  1.1674203 ,\n",
       "        3.1347609 , -0.28568706, -0.09199201,  0.26504394, -0.1727153 ,\n",
       "        1.8427153 ,  0.8492475 , -1.4719603 ,  3.152742  ,  2.0603452 ,\n",
       "       -1.4947088 , -1.7317444 ,  0.42812875, -0.18267067, -1.2561649 ,\n",
       "       -0.79223883, -0.5449633 , -0.13332109, -1.091051  ,  0.53540707,\n",
       "       -1.819616  , -1.5251585 , -2.3003364 , -1.5940654 ,  0.515775  ,\n",
       "        0.3701657 ,  1.7681065 ,  2.529361  ,  0.35616443,  1.2786752 ,\n",
       "        2.1912544 ,  1.3000209 , -0.13651122,  0.30851024,  0.5720359 ,\n",
       "       -0.9974373 , -2.7533169 , -1.6598922 , -0.01443377,  2.2456665 ,\n",
       "       -3.3630264 ,  0.68600214, -0.9684169 , -0.8434749 ,  0.4128957 ,\n",
       "        0.3288789 ,  0.922422  ,  2.214209  ,  1.9684101 , -1.0343835 ,\n",
       "       -0.34593382, -1.1556642 , -2.2290092 ,  3.2235982 ,  0.12422367,\n",
       "        0.6464526 , -0.49824956,  2.2892945 , -1.978824  , -0.76155066,\n",
       "        2.646402  ,  2.1685665 ,  0.5482429 ,  0.20456433, -1.4033229 ,\n",
       "       -0.95026785,  0.5561423 , -2.1360974 , -1.5215931 ,  2.381046  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e5af82cbeb02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = gensim.models.KeyedVectors.load_word2vec_format(\n\u001b[0;32m----> 2\u001b[0;31m     '../word2vec/data/toxic_key_100_5_5.txt')\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../word2vec/data/toxic_key_100_5_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('../word2vec/data/toxic_key_100_5_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4991571 ,  3.274917  ,  2.11796   , -0.1371787 ,  0.38146856,\n",
       "        0.11616096, -0.08705776,  1.3117185 ,  1.3305957 , -0.8239966 ,\n",
       "       -0.31085697, -1.6831822 ,  2.0815713 , -0.8283071 ,  0.7265748 ,\n",
       "       -0.6616854 , -0.6549267 , -1.6704346 , -1.4915667 ,  1.0009212 ,\n",
       "       -1.4669857 , -0.81920934, -0.01954069,  1.0823163 , -1.2473303 ,\n",
       "        0.6664338 ,  1.5074086 ,  0.943202  ,  0.7938697 ,  1.4987851 ,\n",
       "        1.7692456 , -1.6320014 , -0.09104973,  2.1379724 , -1.6890212 ,\n",
       "        2.097513  ,  0.8531938 , -2.1951742 ,  1.6573819 ,  0.6287992 ,\n",
       "       -1.0059637 , -1.5043186 ,  1.9915301 , -0.7913232 ,  0.2882915 ,\n",
       "        4.1284676 , -2.7514691 , -0.49848107, -1.4621027 , -1.5649302 ,\n",
       "        0.04040527, -0.9068644 , -1.0034015 , -1.9706738 , -0.3348441 ,\n",
       "       -1.149053  ,  2.5514214 ,  3.692237  ,  0.6378545 ,  1.7018992 ,\n",
       "        2.3214848 ,  2.4040663 , -0.61251014,  0.01965214, -1.751113  ,\n",
       "        1.1362225 , -1.6848375 , -0.8693423 ,  2.6890762 ,  1.8805321 ,\n",
       "       -3.1145518 , -0.08123364, -1.2204846 , -0.5870361 , -0.34763297,\n",
       "        0.5108362 ,  1.4363767 ,  1.0310589 ,  2.4063654 ,  1.3903389 ,\n",
       "       -0.58876187, -0.6541621 , -2.4751172 ,  3.7639167 , -0.23084454,\n",
       "        2.7315946 ,  0.38317916,  1.5728743 ,  0.01571054,  1.1841946 ,\n",
       "        1.2517575 ,  1.0555265 , -0.89022225, -0.3269367 ,  2.1548717 ,\n",
       "       -0.75260395,  1.5763062 , -1.1766328 , -1.9106121 ,  1.0972658 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
